{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意(attention)！开始做前必读项！\n",
    "\n",
    "- 所有的代码一定要在这个文件里面编写，不要自己创建一个新的文件\n",
    "- 对于提供的数据集，不要改存储地方，也不要修改文件名和内容\n",
    "- 确保到时候git pull之后我们可以直接运行这个 starter_code文件\n",
    "- 不要重新定义函数（如果我们已经定义好的），按照里面的思路来编写。当然，除了我们定义的部分，如有需要可以自行定义函数或者模块\n",
    "- 写完之后，重新看一下哪一部分比较慢，然后试图去优化。一个好的习惯是每写一部分就思考这部分代码的时间复杂度和空间复杂度，AI工程是的日常习惯！\n",
    "- 第一次作业很重要，一定要完成！ 相信会有很多的收获！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: 搭建一个分词工具\n",
    "\n",
    "### Part 1.1  基于枚举方法来搭建中文分词工具\n",
    "\n",
    "此项目需要的数据：\n",
    "1. 综合类中文词库.xlsx： 包含了中文词，当做词典来用\n",
    "2. 以变量的方式提供了部分unigram概率 word_prob\n",
    "\n",
    "\n",
    "举个例子： 给定词典=[我们 学习 人工 智能 人工智能 未来 是]， 另外我们给定unigram概率：p(我们)=0.25, p(学习)=0.15, p(人工)=0.05, p(智能)=0.1, p(人工智能)=0.2, p(未来)=0.1, p(是)=0.15\n",
    "\n",
    "#### Step 1: 对于给定字符串：”我们学习人工智能，人工智能是未来“, 找出所有可能的分割方式\n",
    "- [我们，学习，人工智能，人工智能，是，未来]\n",
    "- [我们，学习，人工，智能，人工智能，是，未来]\n",
    "- [我们，学习，人工，智能，人工，智能，是，未来]\n",
    "- [我们，学习，人工智能，人工，智能，是，未来]\n",
    ".......\n",
    "\n",
    "\n",
    "#### Step 2: 我们也可以计算出每一个切分之后句子的概率\n",
    "- p(我们，学习，人工智能，人工智能，是，未来)= -log p(我们)-log p(学习)-log p(人工智能)-log p(人工智能)-log p(是)-log p(未来)\n",
    "- p(我们，学习，人工，智能，人工智能，是，未来)=-log p(我们)-log p(学习)-log p(人工)-log p(智能)-log p(人工智能)-log p(是)-log p(未来)\n",
    "- p(我们，学习，人工，智能，人工，智能，是，未来)=-log p(我们)-log p(学习)-log p(人工)-log p(智能)-log p(人工)-log p(智能)-log p(是)-log p(未来)\n",
    "- p(我们，学习，人工智能，人工，智能，是，未来)=-log p(我们)-log p(学习)-log p(人工智能)-log p(人工)-log p(智能)-log(是)-log p(未来)\n",
    ".....\n",
    "\n",
    "#### Step 3: 返回第二步中概率最大的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%config ZMQInteractiveShell.ast_node_interactivity='all'\n",
    "import xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataRead(xpath,sheet_index=0):\n",
    "    work_book=xlrd.open_workbook(filename=xpath)\n",
    "    work_sheet=work_book.sheet_by_index(sheet_index)\n",
    "    dic_words={}\n",
    "    max_length=0\n",
    "    for idx in range(work_sheet.nrows):\n",
    "        word=work_sheet.row(idx)[0].value.strip()\n",
    "        temp_lenth=len(word)\n",
    "        if (temp_lenth > max_length):\n",
    "            max_length=temp_lenth\n",
    "        dic_words[word]=0.00001\n",
    "    return dic_words ,max_length\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "# TODO: 第一步： 从dic.txt中读取所有中文词。\n",
    "#  hint: 思考一下用什么数据结构来存储这个词典会比较好？ 要考虑我们每次查询一个单词的效率。 \n",
    "# dic_words ={}     # 保存词典库中读取的单词\n",
    "\n",
    "dic_words,max_length=dataRead(\"综合类中文词库.xlsx\")\n",
    "# 以下是每一个单词出现的概率。为了问题的简化，我们只列出了一小部分单词的概率。 在这里没有出现的的单词但是出现在词典里的，统一把概率设置成为\n",
    "#0.00001\n",
    "# 比如 p(\"学院\")=p(\"概率\")=...0.00001\n",
    "\n",
    "word_prob = {\"北京\":0.03,\"的\":0.08,\"天\":0.005,\"气\":0.005,\"天气\":0.06,\"真\":0.04,\"好\":0.05,\"真好\":0.04,\"啊\":0.01,\"真好啊\":0.02, \n",
    "             \"今\":0.01,\"今天\":0.07,\"课程\":0.06,\"内容\":0.06,\"有\":0.05,\"很\":0.03,\"很有\":0.04,\"意思\":0.06,\"有意思\":0.005,\"课\":0.01,\n",
    "             \"程\":0.005,\"经常\":0.08,\"意见\":0.08,\"意\":0.01,\"见\":0.005,\"有意见\":0.02,\"分歧\":0.04,\"分\":0.02, \"歧\":0.005}\n",
    "# dic_words\n",
    "for key,value in word_prob.items():\n",
    "    dic_words[key]=value\n",
    "print(max_length)\n",
    "print (sum(word_prob.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(segment_recur(\"北京的天气真好啊\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cut_native(input_str):\n",
    "    length_str=len(input_str)\n",
    "\n",
    "    segments=[]\n",
    "    if length_str == 0 :\n",
    "        return segments\n",
    "    \n",
    "    max_split=min(max_length,length_str)+1\n",
    "    for ids in range(1,max_split):\n",
    "        word=input_str[0:ids]\n",
    "        \n",
    "        if word in dic_words:\n",
    "            segment_words=word_cut_native(input_str[ids :]) \n",
    "            if len(segment_words) == 0:\n",
    "                segments.append([word])\n",
    "            else:\n",
    "                for element in segment_words:\n",
    "                    element =[word]+element\n",
    "                    segments.append(element)\n",
    "    return segments\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  分数（10）\n",
    "## TODO 请编写word_segment_naive函数来实现对输入字符串的分词\n",
    "def word_segment_naive(input_str):\n",
    "    \"\"\"\n",
    "    1. 对于输入字符串做分词，并返回所有可行的分词之后的结果。\n",
    "    2. 针对于每一个返回结果，计算句子的概率\n",
    "    3. 返回概率最高的最作为最后结果\n",
    "    \n",
    "    input_str: 输入字符串   输入格式：“今天天气好”\n",
    "    best_segment: 最好的分词结果  输出格式：[\"今天\"，\"天气\"，\"好\"]\n",
    "    \"\"\"\n",
    "    length_input_str=len(input_str)\n",
    "    segments=word_cut_native(input_str)\n",
    "    # TODO： 第一步： 计算所有可能的分词结果，要保证每个分完的词存在于词典里，这个结果有可能会非常多。 \n",
    "#     segments = []  # 存储所有分词的结果。如果次字符串不可能被完全切分，则返回空列表(list)\n",
    "                   # 格式为：segments = [[\"今天\"，“天气”，“好”],[\"今天\"，“天“，”气”，“好”],[\"今“，”天\"，“天气”，“好”],...]\n",
    "    \n",
    "    # TODO: 第二步：循环所有的分词结果，并计算出概率最高的分词结果，并返回\n",
    "    best_segment = []\n",
    "    best_score = np.inf\n",
    "    for seg in segments:\n",
    "        # TODO ...\n",
    "        log_sum= -1 * np.sum(np.log([dic_words[word]+EPSILON  for word in seg]))\n",
    "        if log_sum <best_score:\n",
    "            best_score=log_sum\n",
    "            best_segment=seg\n",
    "    return best_segment      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['北京', '的', '天气', '真好啊']\n",
      "['今天', '的', '课程', '内容', '很有', '意思']\n",
      "['经常', '有意见', '分歧']\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "print (word_segment_naive(\"北京的天气真好啊\"))\n",
    "print (word_segment_naive(\"今天的课程内容很有意思\"))\n",
    "print (word_segment_naive(\"经常有意见分歧\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2  基于维特比算法来优化上述流程\n",
    "\n",
    "此项目需要的数据：\n",
    "1. 综合类中文词库.xlsx： 包含了中文词，当做词典来用\n",
    "2. 以变量的方式提供了部分unigram概率word_prob\n",
    "\n",
    "\n",
    "举个例子： 给定词典=[我们 学习 人工 智能 人工智能 未来 是]， 另外我们给定unigram概率：p(我们)=0.25, p(学习)=0.15, p(人工)=0.05, p(智能)=0.1, p(人工智能)=0.2, p(未来)=0.1, p(是)=0.15\n",
    "\n",
    "#### Step 1: 根据词典，输入的句子和 word_prob来创建带权重的有向图（Directed Graph） 参考：课程内容\n",
    "有向图的每一条边是一个单词的概率（只要存在于词典里的都可以作为一个合法的单词），这些概率已经给出（存放在word_prob）。\n",
    "注意：思考用什么方式来存储这种有向图比较合适？ 不一定只有一种方式来存储这种结构。 \n",
    "\n",
    "#### Step 2: 编写维特比算法（viterebi）算法来找出其中最好的PATH， 也就是最好的句子切分\n",
    "具体算法参考课程中讲过的内容\n",
    "\n",
    "#### Step 3: 返回结果\n",
    "跟PART 1.1的要求一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(input_str):\n",
    "    chars = list(input_str)\n",
    "    chars_length = len(chars)\n",
    "    word_list=[[0 for _ in range(chars_length)] for _ in range(chars_length)]\n",
    "    \n",
    "    for i in range(chars_length):\n",
    "        \n",
    "        for j in range(i,chars_length):\n",
    "            if input_str[i:j+1] in dic_words:\n",
    "                key=input_str[i:j+1]\n",
    "#                 word_list[i][j]=dic_words\n",
    "                word_list[i][j]=dic_words[key]\n",
    "#                 word_list[i][j]=-1 * np.log(dic_words[key])\n",
    "    graph = pd.DataFrame(data=np.array(word_list),index=list(range(chars_length)),columns=list(range(1, chars_length + 1)))\n",
    "    graph = -1 * np.log(graph+EPSILON)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           1          2          3          4          5          6  \\\n",
      "0  11.512915   3.506558  23.025851  23.025851  23.025851  23.025851   \n",
      "1  23.025851  11.512915  23.025851  23.025851  23.025851  23.025851   \n",
      "2  23.025851  23.025851   2.525729  23.025851  23.025851  23.025851   \n",
      "3  23.025851  23.025851  23.025851   5.298317   2.813411  23.025851   \n",
      "4  23.025851  23.025851  23.025851  23.025851   5.298317  23.025851   \n",
      "5  23.025851  23.025851  23.025851  23.025851  23.025851   3.218876   \n",
      "6  23.025851  23.025851  23.025851  23.025851  23.025851  23.025851   \n",
      "7  23.025851  23.025851  23.025851  23.025851  23.025851  23.025851   \n",
      "\n",
      "           7          8  \n",
      "0  23.025851  23.025851  \n",
      "1  23.025851  23.025851  \n",
      "2  23.025851  23.025851  \n",
      "3  23.025851  23.025851  \n",
      "4  23.025851  23.025851  \n",
      "5   3.218876   3.912023  \n",
      "6   2.995732  23.025851  \n",
      "7  23.025851   4.605170  \n"
     ]
    }
   ],
   "source": [
    "print(build_graph(\"北京的天气真好啊\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分数（10）\n",
    "\n",
    "## TODO 请编写word_segment_viterbi函数来实现对输入字符串的分词\n",
    "def word_segment_viterbi(input_str):\n",
    "    \"\"\"\n",
    "    1. 基于输入字符串，词典，以及给定的unigram概率来创建DAG(有向图）。\n",
    "    2. 编写维特比算法来寻找最优的PATH\n",
    "    3. 返回分词结果\n",
    "    \n",
    "    input_str: 输入字符串   输入格式：“今天天气好”\n",
    "    best_segment: 最好的分词结果  输出格式：[\"今天\"，\"天气\"，\"好\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: 第一步：根据词典，输入的句子，以及给定的unigram概率来创建带权重的有向图（Directed Graph） 参考：课程内容\n",
    "    #      有向图的每一条边是一个单词的概率（只要存在于词典里的都可以作为一个合法的单词），这些概率在 word_prob，如果不在word_prob里的单词但在\n",
    "    #      词典里存在的，统一用概率值0.00001。\n",
    "    #      注意：思考用什么方式来存储这种有向图比较合适？ 不一定有只有一种方式来存储这种结构。 \n",
    "    graph = build_graph(input_str)\n",
    "    num_chars=len(input_str)\n",
    "    distances_min=pd.Series( data=np.zeros(shape=(num_chars + 1,)),index=list(range(num_chars + 1)))\n",
    "    path=pd.Series( data=np.zeros(shape=(num_chars + 1,),dtype=np.int),index=list(range(num_chars + 1)))\n",
    "    for idx_end in range(1,num_chars+1):\n",
    "        possible_path_distances = np.zeros(shape=(idx_end, ))\n",
    "        for idx_start in range(idx_end):\n",
    "            possible_path_distances[idx_start] =distances_min[idx_start] + graph.loc[idx_start, idx_end]\n",
    "        distances_min[idx_end] = np.min(possible_path_distances)\n",
    "        path[idx_end] = np.argmin(possible_path_distances)\n",
    "#     print(path)\n",
    "    best_segment=[]\n",
    "    idx = num_chars\n",
    "    while idx > 0:\n",
    "        best_segment.append(input_str[path.loc[idx]: idx])\n",
    "        idx = path.loc[idx]\n",
    "    best_segment.reverse()\n",
    "    \n",
    "#     return best_segment  \n",
    "    # TODO： 第二步： 利用维特比算法来找出最好的PATH， 这个PATH是P(sentence)最大或者 -log P(sentence)最小的PATH。\n",
    "    #              hint: 思考为什么不用相乘: p(w1)p(w2)...而是使用negative log sum:  -log(w1)-log(w2)-...\n",
    "    \n",
    "    # TODO: 第三步： 根据最好的PATH, 返回最好的切分\n",
    "\n",
    "    return best_segment      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['北京', '的', '天气', '真好啊']\n",
      "['今天', '的', '课程', '内容', '很有', '意思']\n",
      "['经常', '有意见', '分歧']\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "print (word_segment_viterbi(\"北京的天气真好啊\"))\n",
    "print (word_segment_viterbi(\"今天的课程内容很有意思\"))\n",
    "print (word_segment_viterbi(\"经常有意见分歧\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 分数（3）\n",
    "# TODO: 第一种方法和第二种方法的时间复杂度和空间复杂度分别是多少？\n",
    "第一个方法： \n",
    "时间复杂度= $O(m^n)$, 空间复杂度=$O(n)$\n",
    "\n",
    "第二个方法：\n",
    "时间复杂度=$O(n^2)$ , 空间复杂度=$O(n^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分数（2）\n",
    "# TODO：如果把上述的分词工具持续优化，有哪些可以考虑的方法？ （至少列出3点）\n",
    "# - 0. （例）， 目前的概率是不完整的，可以考虑大量的语料库，然后从中计算出每一个词出现的概率，这样更加真实\n",
    "# - 1.\n",
    "# - 2.\n",
    "# - 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 2:  搭建一个简单的问答系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次项目的目标是搭建一个基于检索式的简单的问答系统。至于什么是检索式的问答系统请参考课程直播内容/PPT介绍。 \n",
    "\n",
    "通过此项目，你将会有机会掌握以下几个知识点：\n",
    "1. 字符串操作   2. 文本预处理技术（词过滤，标准化）   3. 文本的表示（tf-idf, word2vec)  4. 文本相似度计算  5. 文本高效检索\n",
    "\n",
    "此项目需要的数据：\n",
    "1. dev-v2.0.json: 这个数据包含了问题和答案的pair， 但是以JSON格式存在，需要编写parser来提取出里面的问题和答案。 \n",
    "2. glove.6B: 这个文件需要从网上下载，下载地址为：https://nlp.stanford.edu/projects/glove/， 请使用d=100的词向量\n",
    "\n",
    "##### 检索式的问答系统\n",
    "问答系统所需要的数据已经提供，对于每一个问题都可以找得到相应的答案，所以可以理解为每一个样本数据是 <问题、答案>。 那系统的核心是当用户输入一个问题的时候，首先要找到跟这个问题最相近的已经存储在库里的问题，然后直接返回相应的答案即可。 举一个简单的例子：\n",
    "\n",
    "假设我们的库里面已有存在以下几个<问题,答案>：\n",
    "<\"贪心学院主要做什么方面的业务？”， “他们主要做人工智能方面的教育”>\n",
    "<“国内有哪些做人工智能教育的公司？”， “贪心学院”>\n",
    "<\"人工智能和机器学习的关系什么？\", \"其实机器学习是人工智能的一个范畴，很多人工智能的应用要基于机器学习的技术\">\n",
    "<\"人工智能最核心的语言是什么？\"， ”Python“>\n",
    ".....\n",
    "\n",
    "假设一个用户往系统中输入了问题 “贪心学院是做什么的？”， 那这时候系统先去匹配最相近的“已经存在库里的”问题。 那在这里很显然是 “贪心学院是做什么的”和“贪心学院主要做什么方面的业务？”是最相近的。 所以当我们定位到这个问题之后，直接返回它的答案 “他们主要做人工智能方面的教育”就可以了。 所以这里的核心问题可以归结为计算两个问句（query）之间的相似度。\n",
    "\n",
    "在本次项目中，你会频繁地使用到sklearn这个机器学习库。具体安装请见：http://scikit-learn.org/stable/install.html  sklearn包含了各类机器学习算法和数据处理工具，包括本项目需要使用的词袋模型，均可以在sklearn工具包中找得到。 另外，本项目还需要用到分词工具jieba, 具体使用方法请见 https://github.com/fxsjy/jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1  第一部分： 读取文件，并把内容分别写到两个list里（一个list对应问题集，另一个list对应答案集）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分数（5）\n",
    "def read_corpus():\n",
    "    \"\"\"\n",
    "    读取给定的语料库，并把问题列表和答案列表分别写入到 qlist, alist 里面。 在此过程中，不用对字符换做任何的处理（这部分需要在 Part 2.3里处理）\n",
    "    qlist = [\"问题1\"， “问题2”， “问题3” ....]\n",
    "    alist = [\"答案1\", \"答案2\", \"答案3\" ....]\n",
    "    务必要让每一个问题和答案对应起来（下标位置一致）\n",
    "    \"\"\"\n",
    "    qlist=list()\n",
    "    alist=[]\n",
    "    with open (\"train-v2.0.json\") as file:\n",
    "        data=json.load(file)\n",
    "    for item in data['data']:\n",
    "        for para in item['paragraphs']:\n",
    "            for qa in para['qas']:\n",
    "                qlist.append(qa['question'])\n",
    "            # 部分answers的list为空，所以会引发IndexError\n",
    "                try:\n",
    "                    alist.append(qa['answers'][0]['text'])\n",
    "                except IndexError:\n",
    "                    qlist.pop()\n",
    "        \n",
    "    \n",
    "    assert len(qlist) == len(alist)  # 确保长度一样\n",
    "    return qlist, alist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2 理解数据（可视化分析/统计信息）\n",
    "对数据的理解是任何AI工作的第一步，需要充分对手上的数据有个更直观的理解。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共有 51841 个不同的单词\n",
      "874076\n"
     ]
    }
   ],
   "source": [
    "# 分数（10）\n",
    "# TODO: 统计一下在qlist 总共出现了多少个单词？ 总共出现了多少个不同的单词？\n",
    "#       这里需要做简单的分词，对于英文我们根据空格来分词即可，其他过滤暂不考虑（只需分词）\n",
    "\n",
    "from collections import Counter\n",
    "qlist, alist=read_corpus()\n",
    "word_dict = Counter()\n",
    "for text in qlist:\n",
    "    word_dict.update(text.strip(' .!?').split(' '))\n",
    "    \n",
    "word_total = len(dict(word_dict))\n",
    "# print (\"一共出现了 %d 个单词\"%dif_word_total)\n",
    "print (\"共有 %d 个不同的单词\"%word_total)\n",
    "\n",
    "\n",
    "print(sum(word_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "874076\n"
     ]
    }
   ],
   "source": [
    "print(sum(word_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86821\n"
     ]
    }
   ],
   "source": [
    "print(len(qlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x24478d6fd30>"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2445f02cd30>]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2446ffb6630>"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2445b114e48>]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x24461852f60>"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2446159be10>]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2445cb97400>"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2447941b048>]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共有 51841 个不同的单词\n",
      "[['the', 60960], ['What', 36995], ['of', 33987], ['in', 21785], ['to', 18443], ['was', 17065], ['is', 16198], ['did', 15634], ['what', 13219], ['a', 10753]]\n",
      "874076\n"
     ]
    }
   ],
   "source": [
    "# TODO: 统计一下qlist中每个单词出现的频率，并把这些频率排一下序，然后画成plot. 比如总共出现了总共7个不同单词，而且每个单词出现的频率为 4, 5,10,2, 1, 1,1\n",
    "#       把频率排序之后就可以得到(从大到小) 10, 5, 4, 2, 1, 1, 1. 然后把这7个数plot即可（从大到小）\n",
    "#       需要使用matplotlib里的plot函数。y轴是词频\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "y=[]\n",
    "for i in word_dict:\n",
    "    y.append(word_dict[i])\n",
    "plt.subplot(221)\n",
    "plt.plot(sorted(y,reverse=True))\n",
    "plt.subplot(222)\n",
    "plt.plot(sorted(y,reverse=True)[:2000])\n",
    "plt.subplot(223)\n",
    "plt.plot(sorted(y,reverse=True)[:200])\n",
    "plt.subplot(224)\n",
    "plt.plot(sorted(y,reverse=True)[:20])\n",
    "\n",
    "plt.show()\n",
    "value_sort = sorted(word_dict.values(), reverse=True)    \n",
    "count_qlist_top =dict(zip(word_dict.values(), word_dict.keys()))\n",
    "print (\"共有 %d 个不同的单词\"%word_total)\n",
    "print([[count_qlist_top[v], v] for v in value_sort[:10]])\n",
    "print(sum(word_dict.values()))\n",
    "\n",
    "\n",
    "# counter=Counter()\n",
    "# for text in alist:\n",
    "#     counter.update(text.strip(' .!?').split(' '))\n",
    "# alist_value_sort = sorted(counter.values(), reverse=True)\n",
    "# count_alist_top =dict(zip(counter.values(), counter.keys()))\n",
    "# print([[count_alist_top[v], v] for v in alist_value_sort[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO： 从上面的图中能观察到什么样的现象？ 这样的一个图的形状跟一个非常著名的函数形状很类似，能所出此定理吗？ \n",
    "#       hint: [XXX]'s law\n",
    "# 看出大概形状  齐普夫定律   r*n*常数c  r是排名  n是频次   c是常数\n",
    "# 问题可能同质化  虚词过高  需要去停用词\n",
    "# 排名靠前的有一些没意义\n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 在qlist和alist里出现次数最多的TOP 10单词分别是什么？ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')   \n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 文本预处理\n",
    "次部分需要尝试做文本的处理。在这里我们面对的是英文文本，所以任何对英文适合的技术都可以考虑进来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86821"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分数（10）\n",
    "\n",
    "# TODO: 对于qlist, alist做文本预处理操作。 可以考虑以下几种操作：\n",
    "#       1. 停用词过滤 （去网上搜一下 \"english stop words list\"，会出现很多包含停用词库的网页，或者直接使用NLTK自带的）   \n",
    "#       2. 转换成lo wer_case： 这是一个基本的操作   \n",
    "#       3. 去掉一些无用的符号： 比如连续的感叹号！！！， 或者一些奇怪的单词。\n",
    "#       4. 去掉出现频率很低的词：比如出现次数少于10,20....\n",
    "#       5. 对于数字的处理： 分词完只有有些单词可能就是数字比如44，415，把所有这些数字都看成是一个单词，这个新的单词我们可以定义为 \"#number\"\n",
    "#       6. stemming（利用porter stemming): 因为是英文，所以stemming也是可以做的工作\n",
    "#       7. 其他（如果有的话）\n",
    "#       请注意，不一定要按照上面的顺序来处理，具体处理的顺序思考一下，然后选择一个合理的顺序\n",
    "#  hint: 停用词用什么数据结构来存储？ 不一样的数据结构会带来完全不一样的效率！ \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math\n",
    "sw = set(stopwords.words('english'))\n",
    "sw -= {'who', 'when', 'why', 'where', 'how'}\n",
    "sw.update(['\\'s', '``', '\\'\\''])\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def text_processing(text):\n",
    "    seg=[]\n",
    "    for word in word_tokenize(text):\n",
    "        word =ps.stem(word.lower())\n",
    "        word = \"#number\" if word.isdigit() else word\n",
    "        if len(word)> 1 and word not in sw :\n",
    "            seg.append(word)\n",
    "    return seg\n",
    "counter_qlist=Counter()\n",
    "list_q=[]\n",
    "for text in qlist:\n",
    "    seg=text_processing(text)\n",
    "    list_q.append(seg)\n",
    "    counter_qlist.update(seg)\n",
    "\n",
    "sorted_value=sorted(counter_qlist.values(),reverse=True)\n",
    "min_tf = sorted_value[int(math.exp(0.99 * math.log(len(counter_qlist))))]\n",
    "for  i in range(len(list_q)):\n",
    "    list_q[i] = [word for word in list_q[i] if counter_qlist[word] > min_tf]\n",
    "qlist=list_q\n",
    "# qlist, alist =    # 更新后的\n",
    "len(qlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28170\n"
     ]
    }
   ],
   "source": [
    "print(len(counter_qlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "530774\n"
     ]
    }
   ],
   "source": [
    "print(sum(counter_qlist.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(min_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['when', 'beyonc', 'start', 'becom', 'popular'], ['area', 'beyonc', 'compet', 'when', 'wa', 'grow'], ['when', 'beyonc', 'leav', 'destini', 'child', 'becom', 'solo', 'singer'], ['citi', 'state', 'beyonc', 'grow'], ['decad', 'beyonc', 'becom', 'famou'], ['group', 'wa', 'lead', 'singer'], ['album', 'made', 'worldwid', 'known', 'artist'], ['who', 'manag', 'destini', 'child', 'group'], ['when', 'beyoncé', 'rise', 'fame'], ['role', 'beyoncé', 'destini', 'child']]\n"
     ]
    }
   ],
   "source": [
    "print(qlist[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 在前面步骤里，我们删除了出现次数比较少的单词，那你选择的阈值是多少（小于多少的去掉？）， 这个阈值是根据什么来选择的？ \n",
    "# \n",
    "# 词频的过滤阈值是按照Zipf定律来算的，没见别人这么用过，自己感觉还算合理。\n",
    "\n",
    "# Zipf's law一个实验定律，按照从最常见到非常见排列，第二常见的频率是最常见频率的出现次数的1/2，第三常见的频率是最常见的频率的1/3，\n",
    "# 第n常见的频率是最常见频率出现次数的1/n。\n",
    "\n",
    "# 假设我们文本的词频符合该定律，那么对1/n进行积分得到ln(n)，为了使99%的文本得到覆盖则需ln(x)>0.99*ln(n)，\n",
    "# n是词type数，x是词频从高到底排列时的阈值分割点，最后x=e^(0.99*ln(n))。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 文本表示\n",
    "当我们做完关键的预处理过程之后，就需要把每一个文本转换成向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分数（10）\n",
    "\n",
    "# TODO: 把qlist中的每一个问题字符串转换成tf-idf向量, 转换之后的结果存储在X矩阵里。 X的大小是： N* D的矩阵。 这里N是问题的个数（样本个数），\n",
    "#       D是字典库的大小。 \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer=TfidfVectorizer()\n",
    "X=vectorizer.fit_transform([' '.join(seg) for seg in qlist])\n",
    "\n",
    "\n",
    "# vectorizer =  # 定义一个tf-idf的vectorizer\n",
    "\n",
    "# X =   # 结果存放在X矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86821, 14547)\n",
      "input sparsity ratio: 0.9995936879973627\n"
     ]
    }
   ],
   "source": [
    "# TODO: 矩阵X有什么特点？ 计算一下它的稀疏度\n",
    "\n",
    "def sparsity_ratio(X):\n",
    "    return 1.0 - X.nnz / float(X.shape[0] * X.shape[1])\n",
    "\n",
    "print(X.shape)\n",
    "print(\"input sparsity ratio:\", sparsity_ratio(X))  # 打印出稀疏度(sparsity)\n",
    "# print (sparsity)  # 打印出稀疏度(sparsity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 对于用户的输入问题，找到相似度最高的TOP5问题，并把5个潜在的答案做返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分数（10）\n",
    "\n",
    "\n",
    "from queue import PriorityQueue\n",
    "def top5results(input_q):\n",
    "    \"\"\"\n",
    "    给定用户输入的问题 input_q, 返回最有可能的TOP 5问题。这里面需要做到以下几点：\n",
    "    1. 对于用户的输入 input_q 首先做一系列的预处理，然后再转换成tf-idf向量（利用上面的vectorizer)\n",
    "    2. 计算跟每个库里的问题之间的相似度\n",
    "    3. 找出相似度最高的top5问题的答案\n",
    "    \"\"\"\n",
    "    q_vector = vectorizer.transform([' '.join(text_processing(input_q))])\n",
    "    print(q_vector.shape)\n",
    "    sim = (X * q_vector.T).toarray()\n",
    "    pq = PriorityQueue()\n",
    "    for cur in range(sim.shape[0]):\n",
    "        pq.put((sim[cur][0], cur))\n",
    "        if len(pq.queue) > 5:\n",
    "            pq.get()\n",
    "        pq_rank = sorted(pq.queue, reverse=True, key=lambda x:x[0])\n",
    "    \n",
    "    top_idxs = [x[1] for x in pq_rank]\n",
    "    return [alist[i] for i in top_idxs] \n",
    "#     top_idxs = []  # top_idxs存放相似度最高的（存在qlist里的）问题的下表 \n",
    "#                    # hint: 利用priority queue来找出top results. 思考为什么可以这么做？ \n",
    "    \n",
    "#     return alist[top_idxs]  # 返回相似度最高的问题对应的答案，作为TOP5答案    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airport wa shut']\n"
     ]
    }
   ],
   "source": [
    "ts=[' '.join(text_processing(\"Which airport was shut down?\"))]\n",
    "print(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 14547)\n",
      "['Chengdu Shuangliu International Airport', 'Chengdu Shuangliu International Airport', 'aerodrome with facilities for flights to take off and land', 'newspapers', 'various gaming sites']\n",
      "(1, 14547)\n",
      "['Plymouth City Airport', 'aerodrome with facilities for flights to take off and land', 'related', 'After the reunification', 'Nanjing Dajiaochang Airport']\n",
      "(1, 14547)\n",
      "['Myanmar', 'foreign aid', '10 days', 'the British government', 'The latent heat of water condensation amplifies convection']\n",
      "(1, 14547)\n",
      "['Myanmar', 'Isabel', 'foreign aid', 'Soviet Union and China', '10 days']\n"
     ]
    }
   ],
   "source": [
    "# TODO: 编写几个测试用例，并输出结果\n",
    "print(top5results(\"Which airport was shut down?\"))    # 在问题库中存在，经过对比，返回的首结果正确\n",
    "print(top5results(\"Which airport is closed?\"))\n",
    "print(top5results(\"What government blocked aid after Cyclone Nargis?\"))    # 在问题库中存在，经过对比，返回的首结果正确\n",
    "print(top5results(\"Which government stopped aid after Hurricane Nargis?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分数（5）\n",
    "\n",
    "# TODO: 上面的top5results算法的时间复杂度和空间复杂度分别是多少？\n",
    "\n",
    "时间复杂度 = O()， 空间复杂度 = O()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 利用倒排表的优化。 \n",
    "上面的算法，一个最大的缺点是每一个用户问题都需要跟库里的所有的问题都计算相似度。假设我们库里的问题非常多，这将是效率非常低的方法。 这里面一个方案是通过倒排表的方式，先从库里面找到跟当前的输入类似的问题描述。然后针对于这些candidates问题再做余弦相似度的计算。这样会节省大量的时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    " # 定一个一个简单的倒排表\n",
    "inverted_idx = defaultdict(set)\n",
    "for cur in range(len(qlist)):\n",
    "    for word in qlist[cur]:\n",
    "        inverted_idx[word].add(cur)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分数（10）\n",
    "\n",
    "# TODO: 基于倒排表的优化。在这里，我们可以定义一个类似于hash_map, 比如 inverted_index = {}， 然后存放包含每一个关键词的文档出现在了什么位置，\n",
    "#       也就是，通过关键词的搜索首先来判断包含这些关键词的文档（比如出现至少一个），然后对于candidates问题做相似度比较。\n",
    "# \n",
    "\n",
    "\n",
    "\n",
    "def top5results_invidx(input_q):\n",
    "    \n",
    "    \"\"\"\n",
    "    给定用户输入的问题 input_q, 返回最有可能的TOP 5问题。这里面需要做到以下几点：\n",
    "    1. 利用倒排表来筛选 candidate\n",
    "    2. 对于用户的输入 input_q 首先做一系列的预处理，然后再转换成tf-idf向量（利用上面的vectorizer)\n",
    "    3. 计算跟每个库里的问题之间的相似度\n",
    "    4. 找出相似度最高的top5问题的答案\n",
    "    \"\"\"\n",
    "    input_seg=text_processing(input_q)\n",
    "    candidates = set()\n",
    "    for word in input_seg:\n",
    "    # 取所有包含任意一个词的文档的并集\n",
    "        candidates = candidates | inverted_idx[word]\n",
    "    candidates = list(candidates)\n",
    "    q_vector = vectorizer.transform([' '.join(input_seg)])\n",
    "    sim = (X[candidates] * q_vector.T).toarray()\n",
    "    pq = PriorityQueue()\n",
    "    for cur in range(sim.shape[0]):\n",
    "        pq.put((sim[cur][0], candidates[cur]))\n",
    "        if len(pq.queue) > 5:\n",
    "            pq.get()\n",
    "    \n",
    "    pq_rank = sorted(pq.queue, reverse=True, key=lambda x:x[0])\n",
    "    print([x[0] for x in pq_rank])\n",
    "    top_idxs = [x[1] for x in pq_rank]  # top_idxs存放相似度最高的（存在qlist里的）问题的下表 \n",
    "    \n",
    "    return [alist[i] for i in top_idxs]  # 返回相似度最高的问题对应的答案，作为TOP5答案      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 0.6058098700501424, 0.5955903794475756, 0.5604486086527194]\n",
      "['Chengdu Shuangliu International Airport', 'Chengdu Shuangliu International Airport', 'aerodrome with facilities for flights to take off and land', 'newspapers', 'various gaming sites']\n",
      "[0.7797154765957257, 0.7103241311289762, 0.7038747251719334, 0.6245909883904857, 0.5811739588019266]\n",
      "['Plymouth City Airport', 'aerodrome with facilities for flights to take off and land', 'related', 'After the reunification', 'Nanjing Dajiaochang Airport']\n",
      "[0.9999999999999998, 0.7852110277213404, 0.49331031138548853, 0.4162177525363464, 0.33229596712940707]\n",
      "['Myanmar', 'foreign aid', '10 days', 'the British government', 'The latent heat of water condensation amplifies convection']\n",
      "[0.5947629389746683, 0.39360612204759465, 0.35791876809775003, 0.31237667954304615, 0.29996990837431825]\n",
      "['Myanmar', 'Isabel', 'foreign aid', 'Soviet Union and China', '10 days']\n"
     ]
    }
   ],
   "source": [
    "# TODO: 编写几个测试用例，并输出结果\n",
    "print(top5results_invidx(\"Which airport was shut down?\"))    # 在问题库中存在，经过对比，返回的首结果正确\n",
    "print(top5results_invidx(\"Which airport is closed?\"))\n",
    "print(top5results_invidx(\"What government blocked aid after Cyclone Nargis?\"))    # 在问题库中存在，经过对比，返回的首结果正确\n",
    "print(top5results_invidx(\"Which government stopped aid after Hurricane Nargis?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分数（3）\n",
    "\n",
    "# TODO: 上面的top5results算法的时间复杂度和空间复杂度分别是多少？\n",
    "\n",
    "时间复杂度 = O()， 空间复杂度 = O()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 基于词向量的文本表示\n",
    "上面所用到的方法论是基于词袋模型（bag-of-words model）。这样的方法论有两个主要的问题：1. 无法计算词语之间的相似度  2. 稀疏度很高。 在2.7里面我们\n",
    "讲采用词向量作为文本的表示。词向量方面需要下载： https://nlp.stanford.edu/projects/glove/ （请下载glove.6B.zip），并使用d=100的词向量（100维）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\soft\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  app.launch_new_instance()\n",
      "D:\\soft\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "# _ = glove2word2vec('glove.6B.100d.txt', 'glove2word2vec.6B.100d.txt')\n",
    "model = KeyedVectors.load_word2vec_format('glove2word2vec.6B.100d.txt')\n",
    "def docvec_get(seg):\n",
    "    \"\"\"\n",
    "    将分词数据转为句向量。\n",
    "    seg: 分词后的数据\n",
    "    \n",
    "    return: 句向量\n",
    "    \"\"\"\n",
    "    vector = np.zeros((1, 100))\n",
    "    size = len(seg)\n",
    "    for word in seg:\n",
    "        try:\n",
    "            vector += model.wv[word]\n",
    "        except KeyError:\n",
    "            size -= 1\n",
    "    \n",
    "    return vector / size\n",
    "\n",
    "X = np.zeros((len(qlist), 100))\n",
    "for cur in range(X.shape[0]):\n",
    "    X[cur] = docvec_get(qlist[cur])\n",
    "\n",
    "# 计算X每一行的l2范数\n",
    "Xnorm2 = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "X = X / Xnorm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分数（10）\n",
    "\n",
    "# TODO\n",
    "# emb = # 读取每一个单词的嵌入。这个是 D*H的矩阵，这里的D是词典库的大小， H是词向量的大小。 这里面我们给定的每个单词的词向量，那句子向量怎么表达？\n",
    "      # 其中，最简单的方式 句子向量 = 词向量的平均（出现在问句里的）， 如果给定的词没有出现在词典库里，则忽略掉这个词。\n",
    "\n",
    "def top5results_emb(input_q):\n",
    "    \"\"\"\n",
    "    给定用户输入的问题 input_q, 返回最有可能的TOP 5问题。这里面需要做到以下几点：\n",
    "    1. 利用倒排表来筛选 candidate\n",
    "    2. 对于用户的输入 input_q，转换成句子向量\n",
    "    3. 计算跟每个库里的问题之间的相似度\n",
    "    4. 找出相似度最高的top5问题的答案\n",
    "    \"\"\"\n",
    "\n",
    "    # 用词向量后用词形还原更合理，此处就不做变更了\n",
    "    seg = text_processing(input_q)\n",
    "    # 直接用上边建好的倒排表\n",
    "    candidates = set()\n",
    "    for word in seg:\n",
    "        # 取所有包含任意一个词的文档的并集\n",
    "        candidates = candidates | inverted_idx[word]\n",
    "    candidates = list(candidates)\n",
    "    \n",
    "    q_vector = docvec_get(seg)\n",
    "    # 计算问题向量的l2范数\n",
    "    qnorm2 = np.linalg.norm(q_vector, axis=1, keepdims=True)\n",
    "    q_vector = q_vector / qnorm2\n",
    "    # 计算余弦相似度，前边已经l2规范化过，所以直接相乘\n",
    "    sim = (X[candidates] @ q_vector.T)\n",
    "\n",
    "    # 使用优先队列找出top5\n",
    "    pq = PriorityQueue()\n",
    "    for cur in range(sim.shape[0]):\n",
    "        pq.put((sim[cur][0], candidates[cur]))\n",
    "        if len(pq.queue) > 5:\n",
    "            pq.get()\n",
    "    \n",
    "    pq_rank = sorted(pq.queue, reverse=True, key=lambda x:x[0])\n",
    "    print([x[0] for x in pq_rank])\n",
    "    top_idxs = [x[1] for x in pq_rank]  # top_idxs存放相似度最高的（存在qlist里的）问题的下表 \n",
    "    \n",
    "    return [alist[i] for i in top_idxs]  # 返回相似度最高的问题对应的答案，作为TOP5答案     \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\soft\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 0.8875869259222657, 0.8826214612899685, 0.83558088872733]\n",
      "['Chengdu Shuangliu International Airport', 'Chengdu Shuangliu International Airport', 'Terminal C', 'Nanjing Dajiaochang Airport', '1967']\n",
      "[0.9454294862808652, 0.9029611996952854, 0.9029611996952854, 0.9029611996952854, 0.8917413888585661]\n",
      "['Plymouth City Airport', 'southern suburbs of Paris', 'within the departure areas', 'India', 'Dushanbe International Airport']\n",
      "[1.0, 0.852360897734157, 0.8518187365307015, 0.8508247887568897, 0.8409244964740952]\n",
      "['Myanmar', 'most Protestants (and most Jews)', 'lower house of parliament', 'the Tzu Chi Foundation', 'started an anti-separatist campaign']\n",
      "[0.8828545495470352, 0.8348415264745357, 0.8166760602126991, 0.810772868269737, 0.7993383778232652]\n",
      "['Myanmar', 'the Tzu Chi Foundation', 'started an anti-separatist campaign', 'public gaze', 'most Protestants (and most Jews)']\n"
     ]
    }
   ],
   "source": [
    "# TODO: 编写几个测试用例，并输出结果\n",
    "print(top5results_emb(\"Which airport was shut down?\"))    # 在问题库中存在，经过对比，返回的首结果正确\n",
    "print(top5results_emb(\"Which airport is closed?\"))\n",
    "print(top5results_emb(\"What government blocked aid after Cyclone Nargis?\"))    # 在问题库中存在，经过对比，返回的首结果正确\n",
    "print(top5results_emb(\"Which government stopped aid after Hurricane Nargis?\"))\n",
    "\n",
    "# 我们在验收作业时在后台会建立几个测试用例，来验证返回的准确性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.8 做完本次项目做完之后有什么收获？ \n",
    "\n",
    "#分数（2）\n",
    "\n",
    "回答 = “”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
